{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gridworld\n",
    "import discrete_pendulum\n",
    "from utils import get_learn_function\n",
    "from models.plot import Plot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_x_to_s(env):\n",
    "    theta = np.linspace(-np.pi * (1 - (1 / env.n_theta)), np.pi * (1 - (1 / env.n_theta)), env.n_theta)\n",
    "    thetadot = np.linspace(-env.max_thetadot * (1 - (1 / env.n_thetadot)),\n",
    "                           env.max_thetadot * (1 - (1 / env.n_thetadot)), env.n_thetadot)\n",
    "    for s in range(env.num_states):\n",
    "        i = s // env.n_thetadot\n",
    "        j = s % env.n_thetadot\n",
    "        s1 = env._x_to_s([theta[i], thetadot[j]])\n",
    "        if s1 != s:\n",
    "            raise Exception(f'test_x_to_s: error in state representation: {s} and {s1} should be the same')\n",
    "    print('test_x_to_s: passed')\n",
    "\n",
    "\n",
    "algorithms_for_scenes = {\n",
    "    'gridworld': ['sarsa', 'q_learning', 'policy_iteration', 'value_iteration']}\n",
    "    # 'pendulum': ['sarsa', 'q_learning']}\n",
    "max_it_n = {'sarsa': 2000, 'q_learning': 2000, 'policy_iteration': 200, 'value_iteration': 200}\n",
    "epsilon_fixed = 0.2\n",
    "alpha_fixed = 0.7\n",
    "epsilon_n = [0.1, 0.2, 0.3, 0.4, epsilon_fixed, epsilon_fixed, epsilon_fixed, epsilon_fixed]\n",
    "alpha_n = [alpha_fixed, alpha_fixed, alpha_fixed, alpha_fixed, 0.2, 0.4, 0.6, 0.8]\n",
    "# epsilon_n = [epsilon_fixed]\n",
    "# alpha_n = [alpha_fixed]\n",
    "\n",
    "load_checkpoint = False\n",
    "train = True\n",
    "\n",
    "for scene, algorithms in algorithms_for_scenes.items():\n",
    "    # Create environment\n",
    "    env = gridworld.GridWorld(hard_version=False) if scene == 'gridworld' else discrete_pendulum.Pendulum(n_theta=31,\n",
    "                                                                                                          n_thetadot=31)\n",
    "\n",
    "    if scene == 'pendulum':\n",
    "        test_x_to_s(env)\n",
    "\n",
    "    for alg in algorithms:\n",
    "        max_it = max_it_n[alg]\n",
    "        if alg == 'sarsa' or alg == 'q_learning':\n",
    "            epsilon = epsilon_n\n",
    "            alpha = alpha_n\n",
    "        else:\n",
    "            epsilon = [0]\n",
    "            alpha = [0]\n",
    "\n",
    "        for eps, alp in zip(epsilon, alpha):\n",
    "            env.reset()\n",
    "            learn = get_learn_function(alg=alg)\n",
    "            checkpoint = 'ckp' if load_checkpoint else None\n",
    "            model = learn(env, scene=scene, epsilon=eps, alpha=alp, max_it=max_it, load=checkpoint, train=train)\n",
    "\n",
    "            # model = learn(env, scene='pendulum', epsilon=0.1, alpha=0.5, max_it=2000)\n",
    "            # model.save_checkpoint('ckp')\n",
    "\n",
    "            # Initialize simulation\n",
    "            s = env.reset()\n",
    "\n",
    "            # Create log to store data from simulation\n",
    "            if scene == 'gridworld':\n",
    "                log = {\n",
    "                    't': [0],\n",
    "                    's': [s],\n",
    "                    'a': [],\n",
    "                    'r': [],\n",
    "                }\n",
    "            else:\n",
    "                log = {\n",
    "                    't': [0],\n",
    "                    's': [s],\n",
    "                    'a': [],\n",
    "                    'r': [],\n",
    "                    'theta': [env.x[0]],  # agent does not have access to this, but helpful for display\n",
    "                    'thetadot': [env.x[1]],  # agent does not have access to this, but helpful for display\n",
    "                }\n",
    "\n",
    "            # Simulate until episode is done\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                a = model.get_policy(s)\n",
    "                (s, r, done) = env.step(a)\n",
    "                log['t'].append(log['t'][-1] + 1)\n",
    "                log['s'].append(s)\n",
    "                log['a'].append(a)\n",
    "                log['r'].append(r)\n",
    "                if scene == 'pendulum':\n",
    "                    log['theta'].append(env.x[0])\n",
    "                    log['thetadot'].append(env.x[1])\n",
    "\n",
    "                model.plot.add('trajectory', env.get_pos(s), 'trajectory', alpha=0.5,\n",
    "                               title='Trajectory of ' + model.algorithm + ' in ' + scene, xlabel='step', ylabel='l')\n",
    "\n",
    "            model.plot.plot_policy(model.get_policy(), save=True)\n",
    "            model.plot.plot_state_value_function(model.get_state_value_function(), save=True)\n",
    "            model.plot.plot(save=True)\n",
    "\n",
    "            if scene == 'gridworld':\n",
    "                # Plot data and save to png file\n",
    "                plt.plot(log['t'], log['s'])\n",
    "                plt.plot(log['t'][:-1], log['a'])\n",
    "                plt.plot(log['t'][:-1], log['r'])\n",
    "                plt.legend(['s', 'a', 'r'])\n",
    "                plt.savefig('figures/gridworld/log_' + model.algorithm + '.png')\n",
    "                plt.show()\n",
    "            else:\n",
    "                # Plot data and save to png file\n",
    "                fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "                ax[0].plot(log['t'], log['s'])\n",
    "                ax[0].plot(log['t'][:-1], log['a'])\n",
    "                ax[0].plot(log['t'][:-1], np.array(log['r']) * 200)\n",
    "                ax[0].legend(['s', 'a', 'r'])\n",
    "                ax[1].plot(log['t'], log['theta'])\n",
    "                ax[1].plot(log['t'], log['thetadot'])\n",
    "                ax[1].legend(['theta', 'thetadot'])\n",
    "                plt.savefig('figures/pendulum/log_' + model.algorithm + '.png')\n",
    "                plt.show()\n",
    "\n",
    "            print('Total reward: ', sum(log['r']))\n",
    "\n",
    "    algorithms = []\n",
    "    n_epsilon = 4\n",
    "\n",
    "    from models.base_model import ModelFreeAlg\n",
    "    from models.sarsa.sarsa_learner import SARSA\n",
    "    from models.q_learning.q_learning_learner import QLearning\n",
    "\n",
    "    for alg in [SARSA.alg_type, QLearning.alg_type]:\n",
    "        algorithms = []\n",
    "        for epsilon, alpha in zip(epsilon_n, alpha_n):\n",
    "            algorithms.append(ModelFreeAlg.get_model_free_alg_name([epsilon, alpha, alg]))\n",
    "        Plot.plot_compare([scene for _ in range(n_epsilon)], algorithms[:n_epsilon], key='return_per_episode',\n",
    "                          title=r'Return per episode for different $\\epsilon$ in ' + scene + ' of ' + alg, save=True,\n",
    "                          plot_interval=False)\n",
    "        Plot.plot_compare([scene for _ in range(len(algorithms) - n_epsilon)], algorithms[n_epsilon:],\n",
    "                          key='return_per_episode',\n",
    "                          title=r'Return per episode for different $\\alpha$ in ' + scene + ' of ' + alg, save=True,\n",
    "                          plot_interval=False)\n",
    "\n",
    "    Plot.save_all_plots('figures/data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
